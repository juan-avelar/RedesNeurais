# -*- coding: utf-8 -*-
"""tpf03-4229-Juan.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KFBphdSB8l3igXBBPFQK__tXxv8PjfUG
"""

!pip install evaluate datasets transformers scikit-learn

import os
import numpy as np
import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding,
    get_cosine_schedule_with_warmup
)
import evaluate

# --- 1. Desativar Weights & Biases (Evita pedir senha) ---
os.environ["WANDB_DISABLED"] = "true"

# --- Configuração Inicial ---
MODEL_CHECKPOINT = "distilbert-base-uncased"
TASK = "sst2"

# Carregar Dataset e Métricas
print("Carregando dataset e métricas...")
dataset = load_dataset("glue", TASK)
metric = evaluate.load("glue", TASK)
tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)

# Pré-processamento
def preprocess_function(examples):
    return tokenizer(examples['sentence'], truncation=True, padding=True, max_length=128)

encoded_dataset = dataset.map(preprocess_function, batched=True)

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=labels)

# --- Função Base de Treinamento ---
def run_experiment(exp_name, model_init_fn, training_args_updates={}):
    print(f"\n\n>>> Iniciando Experimento: {exp_name} <<<")

    # Argumentos padrão
    args_dict = {
        "output_dir": f"./results_{exp_name}",
        "eval_strategy": "epoch",  # Atualizado para versao nova
        "save_strategy": "epoch",
        "learning_rate": 2e-5,
        "per_device_train_batch_size": 16,
        "per_device_eval_batch_size": 16,
        "num_train_epochs": 3,
        "weight_decay": 0.01,
        "load_best_model_at_end": True,
        "metric_for_best_model": "accuracy",
        "push_to_hub": False,
        "logging_dir": f"./logs_{exp_name}",
        "optim": "adamw_torch",
        "report_to": "none"  # Garante que nao use W&B
    }

    # Atualiza com modificações específicas
    args_dict.update(training_args_updates)

    args = TrainingArguments(**args_dict)

    model = model_init_fn()

    # Trainer
    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=encoded_dataset["train"],
        eval_dataset=encoded_dataset["validation"],
        processing_class=tokenizer, # Atualizado de 'tokenizer' para 'processing_class'
        compute_metrics=compute_metrics,
    )

    # Treino
    train_result = trainer.train()
    metrics = trainer.evaluate()

    print(f"Resultados ({exp_name}): {metrics}")
    return metrics

# ==========================================
# 1. Reprodução Original (Baseline TPF-02)
# ==========================================
def model_init_original():
    return AutoModelForSequenceClassification.from_pretrained(MODEL_CHECKPOINT, num_labels=2)

# ==========================================
# 2. Modificação 1: Layer Freezing (Arquitetura)
# ==========================================
def model_init_frozen():
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_CHECKPOINT, num_labels=2)

    # Congelar Embeddings
    for param in model.distilbert.embeddings.parameters():
        param.requires_grad = False

    # Congelar as 3 primeiras camadas (0, 1, 2)
    for i in range(3):
        for param in model.distilbert.transformer.layer[i].parameters():
            param.requires_grad = False

    print("Camadas congeladas: Embeddings + Encoder Layers 0-2.")
    return model

# ==========================================
# 3. Modificação 2: Hiperparâmetros (Cosine Scheduler + WD)
# ==========================================
def model_init_hyper():
    return AutoModelForSequenceClassification.from_pretrained(MODEL_CHECKPOINT, num_labels=2)

training_args_mod2 = {
    "learning_rate": 3e-5,
    "weight_decay": 0.05,
    "lr_scheduler_type": "cosine",
    "warmup_ratio": 0.1
}

# --- Execução Principal ---
if __name__ == "__main__":
    print("Rodando Baseline...")
    res_orig = run_experiment("original", model_init_original)

    print("Rodando Modificação 1 (Congelamento)...")
    res_mod1 = run_experiment("layer_freezing", model_init_frozen)

    print("Rodando Modificação 2 (Hiperparâmetros)...")
    res_mod2 = run_experiment("hyperparams_opt", model_init_hyper, training_args_mod2)

    print("\n\n=== RESUMO FINAL ===")
    print(f"Original Acurácia: {res_orig['eval_accuracy']:.4f}")
    print(f"Mod 1 (Freezing) Acurácia: {res_mod1['eval_accuracy']:.4f}")
    print(f"Mod 2 (HyperOpt) Acurácia: {res_mod2['eval_accuracy']:.4f}")